---
layout: post
title: What is eXplainable AI (XAI)?
mathjax: "true"
---
### Introduction

In this blog post, I will shed light on XAI. More precisely, on a possible concept of XAI that mirrors
most of the current research devoted to XAI. But firstly, I would like to introduce some reasons why XAI is important.
Afterward, I will define the possible concept of XAI.

### Importance and potential of XAI
Intuitively XAI is something that should help to understand your model better and eventually to increase trust in your model. Therefore, obtaining accuracy and other performance metrics from a test dataset is already a sort of XAI.
However, is this type of XAI satisfying, especially when wrong predictions could have fatal consequences?
Consider autonomous driving; here, it is quite essential to understand your AI-system thoroughly before using in autonomous driving.

Humans tend to have more compassion towards an object if the object gives explanations for its action.
So, giving AI-systems the ability to explain its predictions could lead to broader acceptance in society [(Mosca 2020)](#2).
Moreover, the [General Data Protection Regulation](https://en.wikipedia.org/wiki/General_Data_Protection_Regulation) also includes a "right to explanation". 
[That could lead to the obligation to be able to explain a deployed ML-model if the model's predictions have an impact on human lives](https://blog.bigml.com/2018/05/01/prediction-explanation-adding-transparency-to-machine-learning/).

XAI could reveal hidden insights that an ML-model has learned from the data. Such knowledge could help researchers to advance in their domains [(Mosca 2020)](#2).

Lastly, in the following figure, we can see the number of publications that, in their title, abstract or keywords, refer to the domain of XAI [(Arrieta et al. 2019)](#1). 
There is an increasing trend in the number of publications devoted to XAI, which could imply the steady growth of XAI importance today and in the future.
<p align="center">
	<img src="/img/interpretable_ai.JPG" alt="geo" width="500" height="300"/>
</p>
<p align="center">
    <em>Figure taken from (Arrieta et al. 2019)</em>
</p>

### Possible definition of XAI

[XAI is a set of concepts](https://en.wikipedia.org/wiki/Explainable_artificial_intelligence) that should help to understand AI's predictions. However, to give a further description of XAI such that it 
addresses all concepts is nearly impossible. For this reason, it might be better to observe just a subset of XAI that could be further described and still cover a large part of XAI. The subset that will be further described
covers *transparent models* and *post-hoc explainability*.  

#### Transparent models

Transparent models are exact opaque of black-box models. 
So, by looking at the definition and parameters of a transparent model, we can understand the model. 
Additionally, we can understand the loss created by running our model on test data [(Mosca, 2020)](#2). 
The figure below nicely depicts the explanation/understanding flow to a user from a model.
<p align="center">
	<img src="/img/transparent_models.JPG" alt="geo" width="600" height="300"/>
</p>
<p align="center">
    <em>Figure taken from (Mosca 2020). The labels "Explanation Flow" was added by myself</em>
</p>

To transparent models belongs, for instance, logistic regressions, or simple decision trees. 

#### Post-hoc explainability
Post-hoc explainability is methods that are independent of the models and the training procedures of the models.
Usually, these methods are applied to an input to understand what were the key features that contributed to a prediction.

Consider an ML-model that was trained to classify bird images.
Now, the model obtains as input the image in the left part of the following figure and classifies it as a bird.
Next, a post-hoc explainability method delivers some explanation of the model's classification.
A possible explanation is depicted in the right part of the figure; the pixels highlighted
in red mainly contributed to the classification. The blue highlighted pixels instead decrease
the probability of the final classification.
<p align="center">
	<img src="/img/shap_explanation.JPG" alt="geo" width="600" height="300"/>
</p>
<p align="center">
    <em>Figure taken from https://github.com/slundberg/shap (Lundberg & Lee, 2017)</em>
</p>

The above explanation was generated using a relatively famous framework for post-hoc explainability called SHAP [(Lundberg & Lee, 2017)](#3).

The figure below depicts the explanation/understanding flow to the user from a post-hoc explainability method.
<p align="center">
	<img src="/img/post-hoc_explainability.JPG" alt="geo" width="600" height="300"/>
</p>
<p align="center">
    <em>Figure taken from (Mosca 2020). The labels "Explanation Flow" was added by myself</em>
</p>

Post-hoc explainability could be further divided into methods that are *model-agnostic* or *model-specific*.
Model-agnostic methods could be applied to any ML-model definition, so such methods do not assume any particular ML-model 
definition. Model-specific methods are tailored to a specific definition of an ML-model;
therefore, they can not be, in general, applied to other types of ML-models  [(Mosca, 2020)](#2).

Usually, a post-hoc explainability method is applied on separate inputs, as shown with the bird image above.
Such methods are called *local*. However, there exist methods that explain the model based on the whole dataset;
such methods are called *global* [(Mosca, 2020)](#2).

To remark there does not exist a clear definition of global post-hoc explainability method,
especially how a final explanation should look like. I am currently writing my master thesis at the [Social Computing Research Group](https://www.in.tum.de/social/group/);
The main goal is to develop possible global explanation methods for hate speech classifiers.
So, stay tuned if you would like to know more about global post-hoc explainability.

All right, that's it; I hope I was able to clarify the term XAI. I recommend [(Mosca, 2020)](#2) or [(Arrieta et al. 2019)](#1)
for a more in-depth theory of XAI. 


## References
<a id="1">(Arrieta et al., 2019)</a> 
Arrieta, A. B., Diaz-Rodriguez, N., Del Ser, J., Bennetot, A., Tabik, S., Barbado, A., Benjamins, R., et al. (2020).
Explainable artificial intelligence (xai): Concepts, taxonomies, opportunities and challenges toward responsible ai.
Information Fusion, 58, 82-115.

<a id="2">(Mosca, 2020)</a>
Mosca, E. (2020), Explainability of Hate Speech Detection Models (Master's thesis, Technical University of Munich, Munich, Germany).

<a id="3">(Lundberg & Lee, 2017)</a>
Lundberg, S. M., & Lee, S.-I. (2017). A unified approach to interpreting model predictions.
In Advances in neural information processing systems (pp. 4765-4774).